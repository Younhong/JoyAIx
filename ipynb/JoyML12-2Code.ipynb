{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 12-2 Deep Neural Network\n",
    "#### Machine Learning with Python by idebtor@gmail.com\n",
    "\n",
    "-----------------------------------\n",
    "\n",
    "## 1. DNN 클래스"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DeepNeuralNet():\n",
    "    \"\"\" implements a deep neural net. \n",
    "        Users may specify any number of layers.\n",
    "        net_arch -- consists of a number of neurons in each layer \n",
    "    \"\"\"\n",
    "    def __init__(self, net_arch, activate = None, \n",
    "                 eta = 1.0, epochs = 100, random_seed = 1):\n",
    "        pass\n",
    "  \n",
    "    def forpass(self, A0):     \n",
    "        pass\n",
    "    \n",
    "    def backprop(self, Z, A, Y):\n",
    "        pass\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        pass   \n",
    "\n",
    "    def predict(self, X):\n",
    "        pass                                     \n",
    "   \n",
    "    def evaluate(self, Xtest, ytest):      \n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. DNN 활성화 함수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Package imports\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import sklearn\n",
    "import sklearn.datasets\n",
    "import sklearn.linear_model\n",
    "# Our own private imports\n",
    "import imp\n",
    "import joy\n",
    "imp.reload(joy)\n",
    "\n",
    "%matplotlib inline\n",
    "np.random.seed(1)   # a good practice for reproducibility and debugging\n",
    "\n",
    "# The following code is used for hiding the warnings and \n",
    "# make this notebook clearer.\n",
    "#import warnings\n",
    "#warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tanh(x):\n",
    "    return (1.0 - np.exp(-2 * x))/(\n",
    "            1.0 + np.exp(-2 * x))\n",
    "def tanh_d(x):\n",
    "    return (1 + tanh(x)) * (1 - tanh(x))\n",
    "\n",
    "def sigmoid(x): \n",
    "    #x = np.clip(x, -500, 500)  \n",
    "    return 1 / (1 + np.exp((-x)))\n",
    "\n",
    "def sigmoid_d(x):\n",
    "    return sigmoid(x) * (1 - sigmoid(x))\n",
    "\n",
    "def relu(x):\n",
    "    return np.maximum(x, 0)\n",
    "\n",
    "def relu_d(x):\n",
    "    x[x<=0] = 0\n",
    "    x[x>0] = 1\n",
    "    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1 DNN 구현: 생성자"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def __init__(self, net_arch, activate = None, eta = 1.0, epochs = 100, random_seed = 1):\n",
    "        self.eta = eta\n",
    "        self.epochs = epochs\n",
    "        self.net_arch = net_arch\n",
    "        self.layers = len(net_arch)\n",
    "        self.W = []\n",
    "        self.random_seed = random_seed\n",
    "        \n",
    "        self.g       = [lambda x: sigmoid(x)   for _ in range(self.layers)]\n",
    "        self.g_prime = [lambda x: sigmoid_d(x) for _ in range(self.layers)]\n",
    "        \n",
    "        if activate is not None:\n",
    "            for i, (g, g_prime) in enumerate(zip(activate[::2], activate[1::2])):\n",
    "                self.g[i+1] = g\n",
    "                self.g_prime[i+1] = g_prime\n",
    "                \n",
    "        for i in range(len(self.g)):\n",
    "            print(type(self.g[i]), id(self.g[i]))\n",
    "        \n",
    "        #print('X.shape={}, y.shape{}'.format(X.shape, y.shape))\n",
    "        # Random initialization with range of weight values (-1,1)\n",
    "        np.random.seed(self.random_seed)\n",
    "        \n",
    "        # A place holder [None] is used to indicated \"unused place\".\n",
    "        self.W = [[None]]    ## the first W0 is not used.\n",
    "        for layer in range(self.layers - 1):\n",
    "            w = 2 * np.random.rand(self.net_arch[layer+1], \n",
    "                                   self.net_arch[layer]) - 1\n",
    "            print('layer:', layer, 'shape:', w.shape)\n",
    "            self.W.append(w)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 DNN 구현: fit() 메소드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit(self, X, y):\n",
    "        print('fit')\n",
    "        self.cost_ = []        \n",
    "        for epoch in range(self.epochs):          \n",
    "            Z, A = self.forpass(X)        \n",
    "            cost = self.backprop(Z, A, y)   \n",
    "            self.cost_.append(\n",
    "                 np.sqrt(np.sum(cost * cost)))    \n",
    "        return self"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3 DNN 구현: 순전파"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " def forpass(self, A0):     \n",
    "        Z = [[None]]   # Z0 is not used.\n",
    "        A = []       # A0 = X0 is used. \n",
    "        A.append(A0)\n",
    "        for i in range(1, len(self.W)):\n",
    "            z = np.dot(self.W[i], A[i-1])\n",
    "            Z.append(z)\n",
    "            a = self.g[i](z)\n",
    "            A.append(a)\n",
    "        return Z, A"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.4 DNN 구현: 역전파"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def backprop(self, Z, A, Y):\n",
    "        # initialize empty lists to save E and dZ\n",
    "        # A place holder None is used to indicated \"unused place\".\n",
    "        E  = [None for x in range(self.layers)]\n",
    "        dZ = [None for x in range(self.layers)]\n",
    "        \n",
    "        # Get error at the output layer or the last layer\n",
    "        ll = self.layers - 1\n",
    "        error = Y - A[ll]\n",
    "        E[ll] = error   \n",
    "        dZ[ll] = error * self.g_prime[ll](Z[ll]) \n",
    "        \n",
    "        # Begin from the back, from the next to last layer\n",
    "        for i in range(self.layers-2, 0, -1):\n",
    "            E[i]  = np.dot(self.W[i+1].T, E[i+1])\n",
    "            dZ[i] = E[i] * self.g_prime[i](Z[i])\n",
    "       \n",
    "        # Adjust the weights, using the backpropagation rules\n",
    "        m = Y.shape[0] # number of samples\n",
    "        for i in range(ll, 0, -1):\n",
    "            self.W[i] += self.eta * np.dot(dZ[i], A[i-1].T) / m\n",
    "        return error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DeepNeuralNet 클래스"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%writefile code/mnistDeepNet.py\n",
    "#%load code/mnistDeepNet.py\n",
    "# deep neural net\n",
    "# version 0.1\n",
    "# author: idebtor@gmail.com \n",
    "\n",
    "import sys\n",
    "class DeepNeuralNet(object):\n",
    "    \"\"\" implements a deep neural net. \n",
    "        Users may specify any number of layers.\n",
    "        net_arch -- consists of a number of neurons in each layer \n",
    "    \"\"\"\n",
    "    def __init__(self, net_arch, activate = None, eta = 1.0, epochs = 100, random_seed = 1):\n",
    "        self.eta = eta\n",
    "        self.epochs = epochs\n",
    "        self.net_arch = net_arch\n",
    "        self.layers = len(net_arch)\n",
    "        self.W = []\n",
    "        self.random_seed = random_seed\n",
    "        \n",
    "        self.g       = [lambda x: sigmoid(x)   for _ in range(self.layers)]\n",
    "        self.g_prime = [lambda x: sigmoid_d(x) for _ in range(self.layers)]\n",
    "        \n",
    "        if activate is not None:\n",
    "            for i, (g, g_prime) in enumerate(zip(activate[::2], activate[1::2])):\n",
    "                self.g[i+1] = g\n",
    "                self.g_prime[i+1] = g_prime\n",
    "                \n",
    "        for i in range(len(self.g)):\n",
    "            print(type(self.g[i]), id(self.g[i]))\n",
    "        \n",
    "        #print('X.shape={}, y.shape{}'.format(X.shape, y.shape))\n",
    "        # Random initialization with range of weight values (-1,1)\n",
    "        np.random.seed(self.random_seed)\n",
    "        \n",
    "        # A place holder [None] is used to indicated \"unused place\".\n",
    "        self.W = [[None]]    ## the first W0 is not used.\n",
    "        for layer in range(self.layers - 1):\n",
    "            w = 2 * np.random.rand(self.net_arch[layer+1], \n",
    "                                   self.net_arch[layer]) - 1\n",
    "            print('layer:', layer, 'shape:', w.shape)\n",
    "            self.W.append(w)  \n",
    "        print('Weight:', self.W)\n",
    "            \n",
    "    def forpass(self, A0):     \n",
    "        Z = [[None]]   # Z0 is not used.\n",
    "        A = []       # A0 = X0 is used. \n",
    "        A.append(A0)\n",
    "        for i in range(1, len(self.W)):\n",
    "            z = np.dot(self.W[i], A[i-1])\n",
    "            Z.append(z)\n",
    "            a = self.g[i](z)\n",
    "            A.append(a)\n",
    "        return Z, A\n",
    "    \n",
    "    def backprop(self, Z, A, Y):\n",
    "        # initialize empty lists to save E and dZ\n",
    "        # A place holder None is used to indicated \"unused place\".\n",
    "        E  = [None for x in range(self.layers)]\n",
    "        dZ = [None for x in range(self.layers)]\n",
    "        \n",
    "        # Get error at the output layer or the last layer\n",
    "        ll = self.layers - 1\n",
    "        error = Y - A[ll]\n",
    "        E[ll] = error   \n",
    "        dZ[ll] = error * self.g_prime[ll](Z[ll]) \n",
    "        \n",
    "        # Begin from the back, from the next to last layer\n",
    "        for i in range(self.layers-2, 0, -1):\n",
    "            E[i]  = np.dot(self.W[i+1].T, E[i+1])\n",
    "            dZ[i] = E[i] * self.g_prime[i](Z[i])\n",
    "       \n",
    "        # Adjust the weights, using the backpropagation rules\n",
    "        m = Y.shape[0] # number of samples\n",
    "        for i in range(ll, 0, -1):\n",
    "            self.W[i] += self.eta * np.dot(dZ[i], A[i-1].T) / m\n",
    "        return error\n",
    "         \n",
    "    def fit(self, X, y):\n",
    "        print('fit')\n",
    "        self.cost_ = []        \n",
    "        for epoch in range(self.epochs):          \n",
    "            Z, A = self.forpass(X)        \n",
    "            cost = self.backprop(Z, A, y)   \n",
    "            self.cost_.append(\n",
    "                 np.sqrt(np.sum(cost * cost)))    \n",
    "        return self\n",
    "\n",
    "    def predict(self, X):\n",
    "        print('predict')\n",
    "        A0 = np.array(X, ndmin=2).T         # A0: inputs\n",
    "        Z, A = self.forpass(A0)     # forpass\n",
    "        return A[-1]                                       \n",
    "   \n",
    "    def evaluate(self, Xtest, ytest):       # fully vectorized calculation\n",
    "        print('evaluate')\n",
    "        m_samples = len(ytest)\n",
    "        scores = 0        \n",
    "        A3 = self.predict(Xtest)\n",
    "        yhat = np.argmax(A3, axis = 0)\n",
    "        scores += np.sum(yhat == ytest)\n",
    "        return scores/m_samples * 100\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. DNN 학습 결과: XOR 테스트"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import joy\n",
    "import matplotlib.pyplot as plt \n",
    "import numpy as np\n",
    "%matplotlib inline\n",
    "\n",
    "# Set the input data and labels for XOR\n",
    "X = np.array([ [0, 0, 1, 1], [0, 1, 0, 1] ])\n",
    "y = np.array([0, 1, 1, 0])\n",
    "\n",
    "# Initialize the deep neural net with\n",
    "dnn = DeepNeuralNet([2, 4, 2, 1], eta = 0.5, epochs = 5000)  \n",
    "\n",
    "# training the deep neural net objcet with X, y\n",
    "dnn.fit(X, y)             \n",
    "\n",
    "joy.plot_decision_regions(X.T, y, dnn)   \n",
    "plt.xlabel('x-axis')\n",
    "plt.ylabel('y-axis')\n",
    "plt.legend(loc='best')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dnn = DeepNeuralNet([2, 4, 1], \n",
    "                    eta = 0.5, epochs = 5000).fit(X, y) \n",
    "plt.plot(range(len(dnn.cost_)), dnn.cost_)\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Squared Sum of Errors')\n",
    "plt.title('DeepNeuralNet:{}'.format(dnn.net_arch))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "X = np.array([ [0, 0, 1, 1], [0, 1, 0, 1] ])\n",
    "y = np.array([0, 1, 1, 0])\n",
    "\n",
    "dnn1 = DeepNeuralNet([2,4,1], eta = 0.5, epochs = 5000).fit(X, y) \n",
    "\n",
    "g = [sigmoid, sigmoid_d, sigmoid, sigmoid_d, sigmoid, sigmoid_d]\n",
    "dnn2 = DeepNeuralNet([2,4,2,1], activate=g, eta = 0.5, epochs = 5000).fit(X, y) \n",
    "plt.plot(range(len(dnn1.cost_)), dnn1.cost_, label='{}'.format(dnn1.net_arch))\n",
    "plt.plot(range(len(dnn2.cost_)), dnn2.cost_, label='{}'.format(dnn2.net_arch))\n",
    "plt.title('DeepNeuralNet for XOR')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Squared Sum of Errors')\n",
    "plt.legend(loc='best')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DNN 학습: 각 층별로 활성화 함수 지정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g1 = [tanh, tanh_d, sigmoid, sigmoid_d, sigmoid, sigmoid_d]\n",
    "dnn1 = DeepNeuralNet([2,4,2,1], activate=g1, eta = 0.5, epochs = 5000).fit(X, y) \n",
    "g2 = [sigmoid, sigmoid_d, sigmoid, sigmoid_d, sigmoid, sigmoid_d]\n",
    "dnn2 = DeepNeuralNet([2,4,2,1], activate=g2, eta = 0.5, epochs = 5000).fit(X, y) \n",
    "plt.plot(range(len(dnn1.cost_)), dnn1.cost_, label='[tanh, sigmoid, sigmoid]')\n",
    "plt.plot(range(len(dnn2.cost_)), dnn2.cost_, label='[sigmoid, sigmoid, sigmoid]')\n",
    "plt.title('DeepNeuralNet: tanh vs sigmoid')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Squared Sum of Errors')\n",
    "plt.legend(loc = 'best')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----------\n",
    "_In the beginning God created the heavens and the earth. Genesis1:1_"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
